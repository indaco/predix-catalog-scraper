#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
    Predix.io Catalog Scraper
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Scrape the Predix.io Catalog and generate an excel file listing
    all the services with detailed information available on it.

    :version: 0.1
    :copyright: 2016 by Mirco Veltri
    :license: GPL, see LICENSE for more details
"""

import os
import sys
import shutil
import signal
from datetime import datetime
import configs  # configuration settings come from configs.py
from utils.webpage_spider import WebPageSpider
from utils.predix_catalog_scraper import PredixCatalogScraper
from utils.excel_file_writer import ExcelFileWriter

OUTPUT_FOLDER = "output"
OUTPUT_FILENAME = "predix-catalog_" + datetime.now().strftime('%Y%m%d') + ".xlsx"

file_writer = ExcelFileWriter()

def services():
    return "services"

def analytics():
    return "analytics"

catalogs = {
    0: services,
    1: analytics
}

def cleanup():
    """ Removing __pycache_folders and clean. """
    ghostdriver_log_file = "ghostdriver.log"
    if os.path.exists(ghostdriver_log_file):
        os.remove(ghostdriver_log_file)

    if os.path.exists("__pycache__"):
        shutil.rmtree("__pycache__")

    if os.path.exists(os.path.join("utils", "__pycache__")):
        shutil.rmtree(os.path.join("utils", "__pycache__"))

    file_writer.close()

def sigint_handler(signum, frame):
    """ Handle CTRL+C in the script. """
    cleanup()

def main():
    """ Main code. """
    # Create an excel workbook
    file_writer.create_file(OUTPUT_FOLDER, OUTPUT_FILENAME)
    # Create a webpage spider instance
    web_spider = WebPageSpider()
    # Create a scraper instance
    scraper = PredixCatalogScraper(web_spider, configs)

    # Scraping the Services Catalog
    print("\n# Scraping Predix Catalog for 'Services':", configs.px_services_catalog_url)
    scraper.set_catalog_name(catalogs[0]())
    scraper.parse()
    services_catalog_categories_counter = scraper.categories_counter()
    services_catalog_tiles_counter = scraper.tiles_counter()

    # Add a worksheet to the Excel file and the content belong it
    file_writer.add_worksheet(catalogs[0]())
    file_writer.set_summary_vars(services_catalog_categories_counter, services_catalog_tiles_counter)
    file_writer.write_content(scraper.get_tiles())

    # reset the tiles array
    scraper.reset()

    # Scraping the Analytics Catalog
    print("\n# Scraping Predix Catalog for 'Analytics':", configs.px_analytics_catalog_url)
    scraper.set_catalog_name(catalogs[1]())
    scraper.parse()
    analytics_catalog_categories_counter = scraper.categories_counter()
    analytics_catalog_tiles_counter = scraper.tiles_counter()

    # Add a worksheet to the Excel file and the content belong it
    file_writer.add_worksheet(catalogs[1]())
    file_writer.set_summary_vars(analytics_catalog_categories_counter, analytics_catalog_tiles_counter)
    file_writer.write_content(scraper.get_tiles())

    # Save and close the Excel file
    print("\n\n* Saving the file...")
    file_writer.close()
    print("\t* '" + os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME) + "' file saved")

    # Output to shell
    print("\n## Summary for 'Services':")
    print("\t* number of available categories:", services_catalog_categories_counter)
    print("\t* number of available services:", services_catalog_tiles_counter)

    print("\n## Summary for 'Analytics':")
    print("\t* number of available categories:", analytics_catalog_categories_counter)
    print("\t* number of available services:", analytics_catalog_tiles_counter)


if __name__ == "__main__":
    signal.signal(signal.SIGINT, sigint_handler)
    # Run as main program
    main()
    # Removing autogenerated files
    cleanup()
